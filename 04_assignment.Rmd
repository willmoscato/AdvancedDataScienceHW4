---
title: 'Assignment #4'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries, message=FALSE}
#Regular expressions
library(tidyverse)        # contains stringr for regex
library(googlesheets4)    # for reading in data
gs4_deauth()              # to read in google sheet (or download)

#tmap
library(tmap)
library(pacman)
library(gifski)          # not needed since you won't do animated graphs

theme_set(theme_minimal()) # Lisa's favorite theme
```

When you finish the assignment, remove the `#` from the options chunk at the top, so that messages and warnings aren't printed. If you are getting errors in your code, add `error = TRUE` so that the file knits. I would recommend not removing the `#` until you are completely finished.

## Put it on GitHub!        

From now on, GitHub should be part of your routine when doing assignments. I recommend making it part of your process anytime you are working in R, but I'll make you show it's part of your process for assignments.

**Task**: When you are finished with the assignment, post a link below to the GitHub repo for the assignment. If you want to post it to your personal website, that's ok (not required). Make sure the link goes to a spot in the repo where I can easily find this assignment. For example, if you have a website with a blog and post the assignment as a blog post, link to the post's folder in the repo. As an example, I've linked to my GitHub stacking material [here](https://github.com/llendway/ads_website/tree/master/_posts/2021-03-22-stacking).

\

[Github Repo](https://github.com/willmoscato/AdvancedDataScienceHW4)

## Regular Expressions

**Tasks:**

Either read in the data using the code below (need to install `googlesheets4` library if you don't have it) or download from [this](
https://drive.google.com/file/d/12oqEt11miNGL_MtIcU75jx84pz6VEq_w/view?usp=sharing) URL and put it in your project folder.

```{r}
bestsellers <- read_sheet("https://docs.google.com/spreadsheets/d/1n3xKHK4-t5S73LgxOJVJWT5fMYjLj7kqmYl1LHkpk80/edit?usp=sharing")
```

**BE SURE TO REMOVE eval=FALSE** from all code sections.

1. Find books with multiple authors (HINT: Consider the possibility of an author having “and” in their name)

```{r}
bestsellers %>% 
  mutate(mult_auth = str_detect(author, "\\sand\\s")) %>% 
  filter(mult_auth == "TRUE") %>% 
  summarize(title, author, mult_auth)
```

2. Detect if the author’s first name starts with a vowel

```{r}
bestsellers %>% 
  mutate(vowel = str_detect(author, "^[AEIOU]")) %>% 
  filter(vowel == "TRUE") %>% 
  summarize(author, vowel)
```

3. Change all authors with the name Alexander to Alex

```{r}
bestsellers %>% 
  mutate(names = str_replace_all(author, "Alexander", "Alex"))
```

4. Find books that are the second book in a series

```{r}
bestsellers %>%
  mutate(series = str_detect(bestsellers %>% pull(description), 
                             pattern = "sequel")) %>%
  filter(series == TRUE)
```

5. Find books that are the third or fourth one in a series

```{r}
bestsellers %>%
  mutate(series = str_detect(bestsellers %>% pull(description), 
                          pattern = "(third|fourth) book")) %>%
  filter(series == TRUE)
```

6. Find books that are the 10th, 11th, ..., or 19th book in a series

```{r}
bestsellers %>%
  mutate(series = str_detect(bestsellers %>% pull(description), 
                          pattern = "1[0-9]th book")) %>%
  filter(series == TRUE)
```

7. Describe in your own words how you would go about writing a regular expression for password pattern matching (ie 8 character minimum, one capital letter minimum, one lowercase letter minimum, one digit minimum, one special character minimum)

\
**I'm not entirely sure, but I think that you would have to use str_locate a bunch of times and have it search for first where the lowercase letters are, then once you have those the uppercase letters, then the numbers, then the special characters.**


## `tmap` exercises

Read in the data:

```{r}
data("World")
```


In addition to the World data, we will use data called **metro** that comes from the `tmap` package. It contains metropolitan area information from major cities across the globe. Some important variables include:

* **Name** : city name
* **pop2010** : population in 2010
* **pop2020** : population in 2020


```{r}
data("metro")
```


**!!!!!!REMEMBER TO REMOVE eval=FALSE!!!!!!!!!!!**

1. Make a world map using the base **World** dataset we used with the COVID example. Add information about income groups (income_grp) to the plot, specify a color palette.


```{r}
# let's explore the variable names of each dataset
names(World)
names(metro)
```


```{r}
tmap_mode('plot')

tm_shape(World) +
    tm_polygons("income_grp", 
                palette="-Blues", 
                contrast= .5, 
                id="name", 
                title="Income group")
```





2. To the plot from (1), add the new dataset **metro** to the plot, and add information about cities' populations in 2020.

```{r}
tmap_mode('plot')

tm_shape(World) +
    tm_polygons("income_grp", palette="-Blues", contrast= .5, id="name", title="Income group") +
  tm_shape(metro) +
  tm_bubbles("pop2020")
```





3. Now, make a new plot with the World data that provides a look at country economic status and the inequality associated with each. 

```{r}
# Set your mode
tmap_mode('plot')

tm_shape(World) +
  tm_polygons("economy") +
  tm_bubbles("inequality", border.alpha = .5) +
  tm_format("World_wide")
```




4. Using a new data set, NDL_muni municipality data from the Netherlands, create a plot with two separate maps.  One showing the percentage of men per municipality of the whole country, and one showing the same but faceted by **province**.


```{r}
tmap_mode("plot")

data(NLD_muni)

NLD_muni$perc_men <- NLD_muni$pop_men / NLD_muni$population * 100

tm1 <- tm_shape(NLD_muni) + tm_polygons("perc_men", palette = 'RdYlBu')

tm2 <- tm_shape(NLD_muni) +
    tm_polygons("perc_men", palette = "RdYlBu") +
    tm_facets(by = "province")

tmap_arrange(tm1, tm2)

```


## Data Ethics: Data visualization principles

I'm hoping that the topic for this week is a review for all of you, but it's good to remind ourselves of these principles.  

**Task:**

Read both short articles in Week6. Data visualization section of [Calling Bulllshit](https://www.callingbullshit.org/syllabus.html#Visual). Were there any principles mentioned that you hadn't heard of before? What graph stood out for you as "the worst"? Did any of the graphs fool you? Or were able to overcome the bad practices (don't worry if they fool you - plently of them have fooled me, too.)? How does practicing good data visualization principles fit in with data ethics?

\
**The worst graph that stood out to me before and has stood out to me in the past is the one showing gun deaths in Florida. The fact that it is upside down makes it seem that the gun deaths are decreasing when they are in fact increasing. The visualization shows the opposite of what the data is saying. I think that the big take aways that I had from both these articles is that you have to pay attention to the trends and the context of the trends that you are trying to show in order to make a visualization that makes sense.**


